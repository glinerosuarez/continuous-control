env_file = "environment/Reacher.app"
train_mode = true
seed = 24
cores = 4
out_dir = "output"
save_freq = 10

[PPO]
gamma = 0.99
lam = 0.97
steps_per_epoch = 4000
clip_ratio = 0.2
target_kl = 0.01
epochs = 2000
max_ep_len = 1000
epochs_mean_rewards = 100 # Number of epochs to compute mean rewards
env_solved_at = 30.0

[ActorCritic]
layers = 3
activation = "tanh"
hidden_nodes = 64
policy_lr = 3e-4
value_lr = 1e-3
train_policy_iters = 80
train_value_iters = 80